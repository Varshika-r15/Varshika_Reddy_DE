{"cells":[{"cell_type":"code","source":["from pyspark.sql import SparkSession\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"37e00590-cb92-4e93-b0f0-40f46bbe13d2","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import expr\n#Concatenate columns\ndata=[(\"James\",\"Bond\"),(\"Scott\",\"Varsa\")] \ndf=spark.createDataFrame(data).toDF(\"col1\",\"col2\") \ndf.withColumn(\"Name\",expr(\" col1 ||','|| col2\")).show()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"1c718892-4db6-49c8-aed5-95a5f2e11172","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-----+-----+-----------+\n| col1| col2|       Name|\n+-----+-----+-----------+\n|James| Bond| James,Bond|\n|Scott|Varsa|Scott,Varsa|\n+-----+-----+-----------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["#Using CASE WHEN sql expression\ndata = [(\"James\",\"M\"),(\"Michael\",\"F\"),(\"Jen\",\"\")]\ncolumns = [\"name\",\"gender\"]\ndf = spark.createDataFrame(data = data, schema = columns)\ndf2 = df.withColumn(\"gender\", expr(\"CASE WHEN gender = 'M' THEN 'Male' \" +\n           \"WHEN gender = 'F' THEN 'Female' ELSE 'unknown' END\"))\ndf2.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"39857b23-d532-4922-bfdb-59e6d6d6ceee","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-------+-------+\n|   name| gender|\n+-------+-------+\n|  James|   Male|\n|Michael| Female|\n|    Jen|unknown|\n+-------+-------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["#Add months from a value of another column\ndata=[(\"2019-01-23\",1),(\"2019-06-24\",2),(\"2019-09-20\",3)] \ndf=spark.createDataFrame(data).toDF(\"date\",\"increment\") \ndf.select(df.date,df.increment,\n     expr(\"add_months(date,increment)\")\n  .alias(\"inc_date\")).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"4a947783-3404-46fe-9938-eafad7958c4c","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+----------+---------+----------+\n|      date|increment|  inc_date|\n+----------+---------+----------+\n|2019-01-23|        1|2019-02-23|\n|2019-06-24|        2|2019-08-24|\n|2019-09-20|        3|2019-12-20|\n+----------+---------+----------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["# Providing alias using 'as'\ndf.select(df.date,df.increment,\n     expr(\"\"\"add_months(date,increment) as inc_date\"\"\")\n  ).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"ca41a300-7b21-49be-89c3-912691015db6","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+----------+---------+----------+\n|      date|increment|  inc_date|\n+----------+---------+----------+\n|2019-01-23|        1|2019-02-23|\n|2019-06-24|        2|2019-08-24|\n|2019-09-20|        3|2019-12-20|\n+----------+---------+----------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["# Add\ndf.select(df.date,df.increment,\n     expr(\"increment + 5 as new_increment\")\n  ).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"0585cd24-845f-42c3-a60b-df7fa805deb9","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+----------+---------+-------------+\n|      date|increment|new_increment|\n+----------+---------+-------------+\n|2019-01-23|        1|            6|\n|2019-06-24|        2|            7|\n|2019-09-20|        3|            8|\n+----------+---------+-------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df.select(\"increment\",expr(\"cast(increment as string) as str_increment\")) \\\n  .printSchema()\n#Use expr()  to filter the rows\ndata=[(100,2),(200,3000),(500,500)] \ndf=spark.createDataFrame(data).toDF(\"col1\",\"col2\") \ndf.filter(expr(\"col1 == col2\")).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"ad44d2cc-d665-4b88-8645-768e6fbf24bc","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- increment: long (nullable = true)\n |-- str_increment: string (nullable = true)\n\n+----+----+\n|col1|col2|\n+----+----+\n| 500| 500|\n+----+----+\n\n"]}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"pyspark-expr 2023-06-13 22:51:24","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
