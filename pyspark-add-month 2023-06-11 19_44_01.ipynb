{"cells":[{"cell_type":"code","source":["from pyspark.sql import SparkSession\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"86c6483f-fa1c-46e3-a0cb-df45a9f6371b","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import col, expr"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"2d5f2e95-4344-45ec-ac5f-d424d485ee6e","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n\nfrom pyspark.sql.functions import col,expr\ndata=[(\"2019-01-23\",1),(\"2019-06-24\",2),(\"2019-09-20\",3)]\nspark.createDataFrame(data).toDF(\"date\",\"increment\") \\\n    .select(col(\"date\"),col(\"increment\"), \\\n      expr(\"add_months(to_date(date,'yyyy-MM-dd'),cast(increment as int))\").alias(\"inc_date\")) \\\n    .show() "],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"43550523-3392-40ae-aff9-78721fcb8974","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+----------+---------+----------+\n|      date|increment|  inc_date|\n+----------+---------+----------+\n|2019-01-23|        1|2019-02-23|\n|2019-06-24|        2|2019-08-24|\n|2019-09-20|        3|2019-12-20|\n+----------+---------+----------+\n\n"]}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"pyspark-add-month 2023-06-11 19:44:01","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
